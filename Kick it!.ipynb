{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from __future__ import division\n",
    "\n",
    "train_input = np.genfromtxt('trainInput.csv', delimiter=' ')\n",
    "train_target = np.genfromtxt('trainTarget.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2567024128686327,\n",
       " 0.4876005361930295,\n",
       " 0.027479892761394103,\n",
       " 0.058981233243967826,\n",
       " 0.08378016085790885,\n",
       " 0.06970509383378017,\n",
       " 0.015750670241286863]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 3.1\n",
    "#Since numpy has no in-built count-function, i simply wrote my own, and did an array-comprehension to report the frequencies in a list\n",
    "def numpycounter(Arr, Att):\n",
    "    acc = 0\n",
    "    for i in range(len(Arr)):\n",
    "        if Arr[i] == Att:\n",
    "            acc+=1\n",
    "    return acc\n",
    "\n",
    "frequencies = [numpycounter(train_target,i)/len(train_target) for i in range(7)]\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Question 3.2\n",
    "#Linear Algebra\n",
    "cov_matrix = np.cov(np.transpose(train_input))\n",
    "eigenval, eigenvec = np.linalg.eig(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting \n",
    "plt.plot((np.cumsum(eigenval)/np.sum(eigenval))[0:20])\n",
    "plt.title('Plot of cummulative variance vs PC\\'s')\n",
    "plt.ylabel('Cummulative variance')\n",
    "plt.xlabel('Used PC\\'s up to 20') #0 means 1 PC and so on\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(eigenval)):\n",
    "    if (np.cumsum(eigenval)/np.sum(eigenval))[i] >= 0.9:\n",
    "        print(i+1) #Because we start at 0\n",
    "        break\n",
    "\n",
    "#15 Principal components needed to explain 90% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only necesarry for preprocessing. However we didn't need to normalize the data for this task.\n",
    "mean_ROI = []\n",
    "for i in range(192):\n",
    "    mean_ROI.append(np.mean(train_input[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting eigenvectors by their largest eigenvalues\n",
    "sorter = np.argsort(eigenval)\n",
    "sorter = np.flipud(sorter)\n",
    "sorted_eigenval = np.real(eigenval[sorter][:])\n",
    "sorted_eigenvec = np.real(eigenvec[:,sorter])\n",
    "\n",
    "#Plotting full spectrum\n",
    "plt.plot(sorted_eigenval)\n",
    "plt.title('Full Eigenspectrum')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.xlabel('Number of eigenvectors')\n",
    "plt.show()\n",
    "\n",
    "#Plotting sub-spectrum\n",
    "plt.plot(sorted_eigenval[0:20])\n",
    "plt.title('Eigenspectrum of first 20 eigenvectors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.xlabel('Number of eigenvectors')\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "plt.show()\n",
    "\n",
    "#Getting the two first PC's\n",
    "two_PC = sorted_eigenvec[:,0:2]\n",
    "two_PC = np.dot(train_input, two_PC)\n",
    "\n",
    "#Colorcoded plotting\n",
    "color = [\"red\",\"yellow\",\"blue\",\"green\",\"purple\", \"orange\", \"black\"]\n",
    "plt.figure(figsize=(10,7))\n",
    "for i in range(7):\n",
    "    plt.scatter(two_PC[train_target==i,0], two_PC[train_target==i,1], c=color[i], label=\"Class: \" + str(i))\n",
    "    plt.legend()\n",
    "plt.title('Color-coded classes')\n",
    "plt.ylabel('Y-axis: PC 2')\n",
    "plt.xlabel('X-axis: PC 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3.3\n",
    "#Computing initial centroids\n",
    "centroids = np.zeros(shape=(7,192))\n",
    "for i in range(7):\n",
    "    centroids[i,:] = train_input[np.where(train_target==i)[0][0]]\n",
    "\n",
    "#Calculating clusters, and projecting them\n",
    "clusters = KMeans(7, centroids, n_init=1)\n",
    "clusters.fit(train_input)\n",
    "finalclusters = clusters.cluster_centers_\n",
    "two_PC_clusters = np.dot(finalclusters, sorted_eigenvec[:,0:2])\n",
    "\n",
    "#Plotting color-coded classes but with the clusters projected down on the first two PC's\n",
    "color = [\"red\",\"yellow\",\"blue\",\"green\",\"purple\", \"orange\", \"black\"]\n",
    "plt.figure(figsize=(10,7))\n",
    "for i in range(7):\n",
    "    plt.scatter(two_PC[train_target==i,0], two_PC[train_target==i,1], c=color[i], label=\"Class: \" + str(i))\n",
    "    plt.plot(two_PC_clusters[i,0], two_PC_clusters[i,1], marker='X', markerfacecolor=color[i], markeredgecolor='black', markersize=15)\n",
    "    plt.legend()\n",
    "plt.title('Color-coded classes')\n",
    "plt.ylabel('Y-axis: PC 2')\n",
    "plt.xlabel('X-axis: PC 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3.5\n",
    "#Using kNN-classifier for nonlinear classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "test_input = np.genfromtxt('testInput.csv', delimiter=' ')\n",
    "test_target = np.genfromtxt('testTarget.csv')\n",
    "\n",
    "#Training the model\n",
    "kNN = KNeighborsClassifier()\n",
    "kNN.fit(train_input,train_target)\n",
    "\n",
    "#Predicting test and training\n",
    "test_prediction = kNN.predict(test_input)\n",
    "train_prediction = kNN.predict(train_input)\n",
    "\n",
    "#Calculating accuracy\n",
    "def classification_error(prediction, labels):\n",
    "    if len(prediction) != len(labels):\n",
    "        print(\"Prediction and labels should have the same dimension.\")\n",
    "        return 0\n",
    "        \n",
    "    acc = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == labels[i]:\n",
    "            acc+=1\n",
    "    return acc/len(prediction)\n",
    "\n",
    "print(\"Train error: \" + str(classification_error(train_prediction, train_target)))\n",
    "print(\"Test error: \" + str(classification_error(test_prediction, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that logistic regression is not a non-linear classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "test_input = np.genfromtxt('testInput.csv', delimiter=' ')\n",
    "test_target = np.genfromtxt('testTarget.csv')\n",
    "\n",
    "def logreg(train_data, train_labels, test_data): \n",
    "    myModel = LogisticRegression()\n",
    "\n",
    "    #Adding the 1-collumn to train\n",
    "    train = np.zeros((len(train_data),len(train_data[0])+1))\n",
    "    train[:,0] = 1\n",
    "    train[:,1:len(train[0])] = train_data[:,0:len(train_data[0])]\n",
    "\n",
    "    #Adding the 1-collumn to test\n",
    "    test = np.zeros((len(test_data),len(test_data[0])+1))\n",
    "    test[:,0] = 1\n",
    "    test[:,1:len(test[0])] = test_data[:,0:len(test_data[0])]\n",
    "\n",
    "    myModel.fit(train,train_labels)\n",
    "    predicted_labels, weights = myModel.predict(test), myModel.coef_[0]\n",
    "\n",
    "    return predicted_labels,weights\n",
    "\n",
    "predicted_labels, weights = logreg(train_input, train_target, test_input)\n",
    "\n",
    "#Computing the accuracy of logreg on test-set\n",
    "corr_acc = 0\n",
    "for i in range(len(test_target)):\n",
    "    if predicted_labels[i] == test_target[i]:\n",
    "        corr_acc +=1\n",
    "print(corr_acc/len(test_target))\n",
    "\n",
    "#Computing the accuracy of logreg on train-set\n",
    "corr_acc = 0\n",
    "predicted_labels2, weights2 = logreg(train_input, train_target, train_input)\n",
    "for i in range(len(train_target)):\n",
    "    if predicted_labels2[i] == train_target[i]:\n",
    "        corr_acc +=1\n",
    "print(corr_acc/len(train_target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
